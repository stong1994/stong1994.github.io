<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>北人</title>
    <link>https://stong1994.github.io/cloudnative/</link>
    <description>Recent content on 北人</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Wed, 17 Aug 2022 21:19:00 +0800</lastBuildDate><atom:link href="https://stong1994.github.io/cloudnative/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Istio可观测性</title>
      <link>https://stong1994.github.io/cloudnative/istio/observe/</link>
      <pubDate>Wed, 17 Aug 2022 21:19:00 +0800</pubDate>
      
      <guid>https://stong1994.github.io/cloudnative/istio/observe/</guid>
      <description>Isito作为服务网格，本身并不提供可观测行的能力，但是Istio可以非常方便的集成这些工具。
可观测性可分为三大块：监控、日志和链路追踪。
监控 Prometheus Prometheus是当下最流行的监控工具，其主要组件包括：
Prometheus server：核心组件，拉取数据并存入时序数据库 Pushgateway：一般情况下，Prometheus Server会主动拉取数据，但是无法适用于生命周期短的任务服务，对于这些服务，Prometheus提供了Pushgateway以供服务进行上报数据。 Service discovery：服务发现组件基本上已经成为微服务时代的标配。 Alertmanager：Prometheus支持自定义指标和报警策略，当触发了配置的条件，则进行报警处理。 Web UI：Prometheus支持客户端通过PromQL来查询数据，常用的开源客户端为Grafana。 Grafana Grafana能够将存储的指标、追踪信息通过可视化的方式展示出来。
Grafana支持多种数据来源，包括：Prometheus、Zipkin、Jaeger、MySQL、Elasticsearch等。
Grafana支持可配置的可视化、自定义的查询，并提供了报警系统。
另外，Grafana还有一系列的”周边“开源项目，如：
Grafana Loki：提供了更丰富的日志堆栈。 Grafana Tempo: 提供了更强大的分布式追踪能力。 Grafana Mimir: 为Prometheus提供了可扩展的长期存储服务。 Kiali Kiali是专用于Istio服务网格的管理工具，其核心功能包括：
可视化网格拓扑结构：通过监控网格中的数据流动来推断网格的拓扑结构，让用户更直观地了解服务之间的调用关系。 健康状态可视化：通过Kiali，可以直观的看到网格中服务的健康状态。 更强大的追踪能力：Kiali集成了Jaeger并提供了更丰富的能力，包括：工作负载可视化、随时间推移而聚合的持续时间指标等等 监控Istio基础设施的状态。 Istio配置工具：提供了web页面来配置Istio，并提供校验能力。 日志 日志采集 日志文件的采集方式有两种，一种是构建单独的日志采集Pod，另一种是在Pod内构建日志采集Sidecar。Filebeat是目前常用的采集工具。
单独的日志采集Pod 基于节点的部署方式，在k8s中，以DaemonSet方式部署，将容器的日志挂载到Filebeat Pod中。
日志采集Sidecar Envoy和Filebeat 部署在同一个Pod内，共享日志数据卷，Envoy 写，Filebeat读，实现对Envoy 访问日志的采集。
ELK Stack ELK是三种工具的简称：
Elasticarch: 开源分布式搜索引擎，提供搜集、分析、存储数据三大功能。 Logstash：数据处理工具，将多个数据源采集到的数据进行解析、转换，并存储到指定的数据库中。 Kibana：具有日志分析、查询、汇总等功能的web管理端。 EFK 区别于ELK，EFK使用Fluentd替代了Logstash，更准确的说，是替代了Logstash+Filebeat。
不同类型的、不同来源的日志都通过Fluentd进行统一的日志聚合和处理，并发送到后端进行存储，实现了较小的资源消耗和高性能的处理。
链路追踪 链路追踪已经成为微服务时代的不可缺少的组件。当一个系统中的微服务往往分配给多个开发人员维护，每个开发人员只能了解自己负责的服务逻辑，对于一个请求的整体链路缺少认知。通过链路追踪，开发人员能够更清晰的了解一个请求的整体面貌。
OpenTracing &amp;amp; Jaeger OpenTracing是一个项目，也是一套规范，这套规范已经成为了云原生下链路追踪的实现标准。重点概念包括：
Trace：一个请求从开始到结束的整个过程。 Span：一个追踪单位，由名称和一段时间来定义，一个Trace由多个Span组成。 Span Context：一次追踪中的上下文信息，包括TraceID、SpanID以及存储的log等信息。在服务之间调用时，往往将信息进行序列化存储在请求头部，接受服务接收到请求后将信息提取出来，并构建自己的Span Context。 Jaeger已经成为了OpenTracing首选的实现组件，OpenTracing官网使用的项目正是Jaeger。其他实现了OpenTracing的开源项目有：Zipkin、SkyWalking等。
架构 Jaeger的整体架构由以下部分组成：
jaeger-client：通过代码在服务内部推送Jaeger数据到jaeger-agent，社区内已实现了常用语言的框架，开发能够以非常低的成本进行接入。 jaeger-agent：收集agent-client推送的数据，并推送到jager-collector。jaeger-agent可以单独部署在pod中，也可以直接部署在container中（以边车的方式）。 jaeger-collector：接受agent发送的数据，验证追踪数据并建立索引，最后异步存入数据库 DB：链路数据存储器，支持内存、Cassandra、Elasticsearch、Kafka。 UI：主要的用户交互页面，用于查询、展示数据。 </description>
    </item>
    
    <item>
      <title>Istio流量治理</title>
      <link>https://stong1994.github.io/cloudnative/istio/traffic/</link>
      <pubDate>Thu, 11 Aug 2022 13:16:00 +0800</pubDate>
      
      <guid>https://stong1994.github.io/cloudnative/istio/traffic/</guid>
      <description>流量治理是Istio的核心功能，也是其“立身之本”。
在上一篇《Istio总览》中，我们可以看到，Istio分为控制平面和数据平面，流量控制属于数据平面。
流量控制CRD Istio的流量控制是通过一系列的CRD（Kubernetes的自定义资源）实现的，包括VirtualService、DestinationRule、ServiceEntry、Gateway和Sidecar五个资源。
VirtualService VirtualService 用于控制流量转发规则及API 粒度治理功能，包括错误注入、域控制等。
VirtualService中定义了一组路由规则，当流量进入时，逐个规则进行匹配，直到匹配成功后将流量转发给指定的路由地址。
DestinationRule DestinationRule 用于抽象路由转发的上游目标服务及其相关配置项，如负载均衡配置、目标服务连接池配置、熔断配置、健康检查配置等。用户DestinationRule 可以将目标服务根据特定标签匹配划分为多个子集。
DestinationRule在VirtualService的路由规则之后起作用。
ServiceEntry ServiceEntry可以将网格外的服务注册到Istio的注册表中，这样就可以把外部服务当作和网格内部的服务一样进行管理和操作，包括服务发现、路由控制等。
Gateway Gateway 一般用于 Ingress 和Egress，定义了服务网格的出入口及相关治理规则。通俗来说，Gateway 会控制一些特定的工作负载打开一个公开的监听端口，供服务网格内外部业务直接访问。再配合 VirtualService 和 DestinationRule 等CRD，可以对到达该端口的流量做一系列的管理和观察。
Gateway配置应用于网格边缘的独立的Envoy代理上，而不是服务负载的Envoy代理上。
Sidecar Sidecar用于声明服务网格中服务间的依赖关系。一般来说，网格数据平面为了代理网格流量，只需要了解其所依赖的少量服务的状态、治理配置、目标地址等信息即可。但是，因为服务网格不了解服务间依赖关系，所以默认服务网格会将所有配置都推送给网格数据平面，从而带来内存开销膨胀的问题。而 Sidecar 则可以显式指定服务间的依赖关系，改善内存开销问题。
灰度发布 灰度发布是指将流量按比例分配到不同的服务，常用的场景有：AB测试、金丝雀发布、
实现灰度发布仅需在ServiceEntry中对不同的Destination配置对应的比重即可，但是通常来说，需要配合DestinationRule为目标服务根据标签来划分为多个子集。
流量镜像 流量镜像（Mirroring /Traffic Shadow），也被称为影子流量，可以通过一定的配置将线上的真实流量复制一份到镜像服务中，并通过流量镜像转发，从而达到在不影响线上服务的情况下对流量或请求内容做具体分析的目的。它的设计思想是只做转发而不接收响应。
传统的测试手段无法满足复杂的现实场景，因此在项目上线前使用真实的流量进行服务检测，能够最大化的找到潜在的未知问题。
当遇到复杂的问题难以排查时，往往需要进行代码调试，而线上环境往往是不可调试的，因此，可以通过流量镜像将流量达到临时的环境中进行调试。
流量镜像并不是备份流量，只是将流量复制到一个新的服务。
超时控制 服务之前的请求需要设置超时时间，这样可以避免级联现象导致的“雪崩”。一般情况下我们都会在服务实例的层面上控制请求的超时时间，这样可以做到针对不同的请求设置不同的超时时间，但是这样做存在弊端：
繁琐：为每个服务请求都设置一个超时时间，这是一个繁琐且枯燥的的工作，可以通过封装通用的工具来解决这个问题。 容易遗漏：在服务数量巨大、开发人员质量参差不齐的情况下，很容易造成一些调用漏掉了超时时间，这就为未来发生的故障埋下了种子。 通过在网格数据平面拦截流量来统一实现超时控制，这能够大大降低开发人员的心智负担。但是新的问题也随之而来——有些请求本身就很慢怎么办？
这是一个很复杂的问题，不同的场景有不同的解决方案，如增加缓存、在数据平面增加重试、在业务上改造等等。
重试机制 重试机制的意义在于避免了某种偶发情况（如网络波动）导致的服务故障，其目的在于提升用户体验。
重试机制带来的问题是对服务层面的幂等性要求，当然，这不是数据平面带来的问题，普通的服务间访问也会有重试机制，但是开发人员需要明确自己的服务会不会受重试机制的影响。
熔断机制 熔断是系统的自我保护机制，就像家里的电器一样，当“断路器”发现整体电压过大（服务故障频发），就会进行“断路”操作，避免引发更大的灾难。
在微服务中，系统中的某些模块发生故障后，可以通过降级等方式来提升整体系统的可用性。
故障注入 系统的健壮性不是建立在完善的测试用例上，而在于处理各种意外情况的能力。因此，在对系统整体测试时，需要模拟各种情况下的服务故障。传统的方式（如手动停服务、改代码）的成本很高，操作也很复杂，Istio中提供了一种无侵入的注入故障的能力。
这些能力包括：模拟上游服务的处理时长、服务异常状态、自定义响应状态码等故障信息，暂不支持对服务主机内存、CPU等信息故障的模拟。实现方式则是通过配置上游主机的VirtualService实现的。</description>
    </item>
    
    <item>
      <title>Istio总览</title>
      <link>https://stong1994.github.io/cloudnative/istio/overview/</link>
      <pubDate>Sun, 07 Aug 2022 21:19:00 +0800</pubDate>
      
      <guid>https://stong1994.github.io/cloudnative/istio/overview/</guid>
      <description>什么是Istio Istio是一个服务网格形态的平台，用于治理云原生中的服务。
治理什么 微服务带来了更高的可用性、可维护性等一系列好处，同时也带来了更复杂的服务调用，复杂的服务调用导致了流量控制非常繁琐，而Istio的使命就是让流量控制更简单。
如何治理 治理手段应尽量避免对服务代码的侵入，否则维护成本会非常高。Istio是一个服务网格形态的平台，通过边车代理的方式实现了对服务实例的流量的管控。
Istio平台整体上可分为两部分：
控制平面：Istio平台的中央控制器，负责维护配置信息、响应管理员并控制整个网络。 数据平面：拦截服务实例的流量，并根据控制平面下发的配置来管控流量。 控制平面 控制平面的职责是管理数据平面中的边车代理，完成服务发现、配置分发、授权鉴权等功能。
Pilot Pilot是控制控制平面的中枢系统，用于管理和配置部署在Istio数据平面的边车代理。
抽象模型：为了实现对不同服务注册中心（Kubernetes、Consul等）的支持，Pilot需要对不同来源的输入数据进行统一格式的存储，即抽象模型。 平台适配器： Pilot的实现是基于平台适配器的，借助平台适配器Pilot可以实现服务注册中心数据和抽象模型数据之间的转换。 xDS API：Pilot使用了一套源于Envoy项目的标准数据平面API，将服务信息和流量规则下发到数据平面的Sidecar中。这套标准数据平面API，也被称为 xDS。 LDS，Listener发现服务：Listener控制Sidecar启动端口监听（目前支持的协议只有TCP），并配置L3/L4层过滤器，当网络连接完成时，配置好的网络过滤器堆栈开始处理后续事件。 RDS，Router发现服务：用于HTTP连接管理过滤器动态获取路由配置，路由配置包含HTTP 头部修改（增加、删除HTTP 头部键值）、Virtual Hosts（虚拟主机），以及Virtual Hosts定义的各个路由条目。 CDS，Cluster发现服务：用于动态获取Cluster信息。 EDS，Endpoint发现服务：用于动态维护端点信息，端点信息中还包括负载均衡权重、金丝雀状态等。基于这些信息，Sidecar可以做出智能的负载均衡决策。 Citadel Citadel是Istio中负责身份认证和证书管理的核心安全组件，主要包括CA服务器、SDS服务器、证书密钥控制器和证书轮换等模块。
CA服务器 Citadel中的CA 签发机构是一个gRPC服务器，启动时会注册两个gRPC服务：一个是CA服务，用来处理CSR请求（Certificate Signing Request）；另一个是证书服务，用来签发证书。CA 首先通过HandleCSR接口处理来自客户端的CSR请求，然后对客户端进行身份认证（包括TLS认证和JWT认证），认证成功后会调用CreateCertificate进行证书签发。
安全发现服务器（SDS） SDS是一种在运行时动态获取证书私钥的API，Istio中的SDS服务器负责证书管理，并实现了安全配置的自动化。
证书密钥控制器 证书密钥控制器可以监听istio.io/key-and-cert类型的Secret资源，还会周期性地检查证书是否过期，并更新证书。
证书轮换 Istio通过一个轮换器（Rotator）自动检查自签名的根证书，并在证书即将过期时进行更新。它本质上是一个协程（Goroutine），在后台轮询中实现。
秘钥和证书的轮换过程 Envoy通过SDS API发送证书和密钥请求。 istio-agent作为Envoy的代理，创建一个私钥和证书签名请求（CSR），并发送给istiod。 证书签发机构验证收到CSR并生成证书。 istio-agent将私钥和从istiod中收到的证书通过SDS API发送给Envoy。 Galley Galley是整个控制平面的配置管理中心，负责配置校验、管理和分发。Galley可以使用网格配置协议（Mesh Configuration Protocol）和其他组件进行配置的交互。Galley解决了各个组件“各自为政”导致的可复用度低、缺乏统一管理、配置隔离、ACL管理等方面的问题。
MCP协议 MCP提供了一套用于配置订阅和分发的API，这些API在MCP中可以被抽象为以下模型。
source：“配置”的提供端，在Istio中，Galley即source。 sink：“配置”的消费端，在Istio中，典型的sink包括Pilot和Mixer组件。 resource：source和sink关注的资源体，就是Istio中的“配置”。 数据平面 Istio数据平面核心是以Sidecar模式运行的智能代理。Sidecar模式将数据平面核心组件部署到单独的流程或容器中，以提供隔离和封装。
数据平面的Sidecar代理可以调节和控制微服务之间所有的网络通信，每个服务Pod在启动时会伴随启动istio-init和Proxy容器。其中，istio-init容器的主要功能是初始化Pod 网络和对Pod设置iptable规则，在设置完成后自动结束。
数据平面的功能 服务发现：探测所有可用的上游或后端服务实例。 健康检测：探测上游或后端服务实例是否健康，是否准备好接收网络流量。 流量路由：将网络请求路由到正确的上游或后端服务。 负载均衡：在对上游或后端服务进行请求时，选择合适的服务实例接收请求，同时负责处理超时、断路、重试等情况。 身份认证和授权：在istio-agent与istiod的配合下，对网络请求进行身份认证、权限认证，以决定是否响应及如何响应，还可以使用mTLS或其他机制对链路进行加密等。 链路追踪：对每个请求生成详细的统计信息、日志记录和分布式追踪数据，以便操作人员能够明确调用路径并在出现问题时进行调试。 数据平面实现 Envoy：Istio默认使用的数据平面实现方案，使用C++开发，性能较高。 MOSN：由阿里巴巴公司开源，设计类似 Envoy，使用Go 语言开发，优化了过多协议支持的问题。 Linkerd：一个提供弹性云原生应用服务网格的开源项目，也是面向微服务的开源RPC代理，使用Scala开发。它的核心是一个透明代理，因此也可作为典型的数据平面实现方案。 Any Question？ 边车模式使得网络请求在每次服务访问中都增加了两跳（进入服务前被拦截&amp;amp;从服务出来后又被拦截），这会不会对整体的系统性能造成影响？</description>
    </item>
    
  </channel>
</rss>
