<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>北人</title>
    <link>https://stong1994.github.io/cloudnative/</link>
    <description>Recent content on 北人</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Sun, 07 Aug 2022 21:19:00 +0800</lastBuildDate><atom:link href="https://stong1994.github.io/cloudnative/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Istio流量治理</title>
      <link>https://stong1994.github.io/cloudnative/istio/traffic/</link>
      <pubDate>Sun, 07 Aug 2022 21:19:00 +0800</pubDate>
      
      <guid>https://stong1994.github.io/cloudnative/istio/traffic/</guid>
      <description>流量治理是Istio的核心功能，也是其“立身之本”。
在上一篇《Istio总览》中，我们可以看到，Istio分为控制平面和数据平面，流量控制属于数据平面。
流量控制CRD Istio的流量控制是通过一系列的CRD（Kubernetes的自定义资源）实现的，包括VirtualService、DestinationRule、ServiceEntry、Gateway和Sidecar五个资源。
VirtualService VirtualService 用于控制流量转发规则及API 粒度治理功能，包括错误注入、域控制等。
VirtualService中定义了一组路由规则，当流量进入时，逐个规则进行匹配，直到匹配成功后将流量转发给指定的路由地址。
DestinationRule DestinationRule 用于抽象路由转发的上游目标服务及其相关配置项，如负载均衡配置、目标服务连接池配置、熔断配置、健康检查配置等。用户DestinationRule 可以将目标服务根据特定标签匹配划分为多个子集。
DestinationRule在VirtualService的路由规则之后起作用。
ServiceEntry ServiceEntry可以将网格外的服务注册到Istio的注册表中，这样就可以把外部服务当作和网格内部的服务一样进行管理和操作，包括服务发现、路由控制等。
Gateway Gateway 一般用于 Ingress 和Egress，定义了服务网格的出入口及相关治理规则。通俗来说，Gateway 会控制一些特定的工作负载打开一个公开的监听端口，供服务网格内外部业务直接访问。再配合 VirtualService 和 DestinationRule 等CRD，可以对到达该端口的流量做一系列的管理和观察。
Gateway配置应用于网格边缘的独立的Envoy代理上，而不是服务负载的Envoy代理上。
Sidecar Sidecar用于声明服务网格中服务间的依赖关系。一般来说，网格数据平面为了代理网格流量，只需要了解其所依赖的少量服务的状态、治理配置、目标地址等信息即可。但是，因为服务网格不了解服务间依赖关系，所以默认服务网格会将所有配置都推送给网格数据平面，从而带来内存开销膨胀的问题。而 Sidecar 则可以显式指定服务间的依赖关系，改善内存开销问题。
灰度发布 灰度发布是指将流量按比例分配到不同的服务，常用的场景有：AB测试、金丝雀发布、
实现灰度发布仅需在ServiceEntry中对不同的Destination配置对应的比重即可，但是通常来说，需要配合DestinationRule为目标服务根据标签来划分为多个子集。
流量镜像 流量镜像（Mirroring /Traffic Shadow），也被称为影子流量，可以通过一定的配置将线上的真实流量复制一份到镜像服务中，并通过流量镜像转发，从而达到在不影响线上服务的情况下对流量或请求内容做具体分析的目的。它的设计思想是只做转发而不接收响应。
传统的测试手段无法满足复杂的现实场景，因此在项目上线前使用真实的流量进行服务检测，能够最大化的找到潜在的未知问题。
当遇到复杂的问题难以排查时，往往需要进行代码调试，而线上环境往往是不可调试的，因此，可以通过流量镜像将流量达到临时的环境中进行调试。
流量镜像并不是备份流量，只是将流量复制到一个新的服务。
超时控制 服务之前的请求需要设置超时时间，这样可以避免级联现象导致的“雪崩”。一般情况下我们都会在服务实例的层面上控制请求的超时时间，这样可以做到针对不同的请求设置不同的超时时间，但是这样做存在弊端：
繁琐：为每个服务请求都设置一个超时时间，这是一个繁琐且枯燥的的工作，可以通过封装通用的工具来解决这个问题。 容易遗漏：在服务数量巨大、开发人员质量参差不齐的情况下，很容易造成一些调用漏掉了超时时间，这就为未来发生的故障埋下了种子。 通过在网格数据平面拦截流量来统一实现超时控制，这能够大大降低开发人员的心智负担。但是新的问题也随之而来——有些请求本身就很慢怎么办？
这是一个很复杂的问题，不同的场景有不同的解决方案，如增加缓存、在数据平面增加重试、在业务上改造等等。
重试机制 重试机制的意义在于避免了某种偶发情况（如网络波动）导致的服务故障，其目的在于提升用户体验。
重试机制带来的问题是对服务层面的幂等性要求，当然，这不是数据平面带来的问题，普通的服务间访问也会有重试机制，但是开发人员需要明确自己的服务会不会受重试机制的影响。
熔断机制 熔断是系统的自我保护机制，就像家里的电器一样，当“断路器”发现整体电压过大（服务故障频发），就会进行“断路”操作，避免引发更大的灾难。
在微服务中，系统中的某些模块发生故障后，可以通过降级等方式来提升整体系统的可用性。
故障注入 系统的健壮性不是建立在完善的测试用例上，而在于处理各种意外情况的能力。因此，在对系统整体测试时，需要模拟各种情况下的服务故障。传统的方式（如手动停服务、改代码）的成本很高，操作也很复杂，Istio中提供了一种无侵入的注入故障的能力。
这些能力包括：模拟上游服务的处理时长、服务异常状态、自定义响应状态码等故障信息，暂不支持对服务主机内存、CPU等信息故障的模拟。实现方式则是通过配置上游主机的VirtualService实现的。</description>
    </item>
    
    <item>
      <title>Istio总览</title>
      <link>https://stong1994.github.io/cloudnative/istio/overview/</link>
      <pubDate>Sun, 07 Aug 2022 21:19:00 +0800</pubDate>
      
      <guid>https://stong1994.github.io/cloudnative/istio/overview/</guid>
      <description>什么是Istio Istio是一个服务网格形态的平台，用于治理云原生中的服务。
治理什么 微服务带来了更高的可用性、可维护性等一系列好处，同时也带来了更复杂的服务调用，复杂的服务调用导致了流量控制非常繁琐，而Istio的使命就是让流量控制更简单。
如何治理 治理手段应尽量避免对服务代码的侵入，否则维护成本会非常高。Istio是一个服务网格形态的平台，通过边车代理的方式实现了对服务实例的流量的管控。
Istio平台整体上可分为两部分：
控制平面：Istio平台的中央控制器，负责维护配置信息、响应管理员并控制整个网络。 数据平面：拦截服务实例的流量，并根据控制平面下发的配置来管控流量。 控制平面 控制平面的职责是管理数据平面中的边车代理，完成服务发现、配置分发、授权鉴权等功能。
Pilot Pilot是控制控制平面的中枢系统，用于管理和配置部署在Istio数据平面的边车代理。
抽象模型：为了实现对不同服务注册中心（Kubernetes、Consul等）的支持，Pilot需要对不同来源的输入数据进行统一格式的存储，即抽象模型。 平台适配器： Pilot的实现是基于平台适配器的，借助平台适配器Pilot可以实现服务注册中心数据和抽象模型数据之间的转换。 xDS API：Pilot使用了一套源于Envoy项目的标准数据平面API，将服务信息和流量规则下发到数据平面的Sidecar中。这套标准数据平面API，也被称为 xDS。 LDS，Listener发现服务：Listener控制Sidecar启动端口监听（目前支持的协议只有TCP），并配置L3/L4层过滤器，当网络连接完成时，配置好的网络过滤器堆栈开始处理后续事件。 RDS，Router发现服务：用于HTTP连接管理过滤器动态获取路由配置，路由配置包含HTTP 头部修改（增加、删除HTTP 头部键值）、Virtual Hosts（虚拟主机），以及Virtual Hosts定义的各个路由条目。 CDS，Cluster发现服务：用于动态获取Cluster信息。 EDS，Endpoint发现服务：用于动态维护端点信息，端点信息中还包括负载均衡权重、金丝雀状态等。基于这些信息，Sidecar可以做出智能的负载均衡决策。 Citadel Citadel是Istio中负责身份认证和证书管理的核心安全组件，主要包括CA服务器、SDS服务器、证书密钥控制器和证书轮换等模块。
CA服务器 Citadel中的CA 签发机构是一个gRPC服务器，启动时会注册两个gRPC服务：一个是CA服务，用来处理CSR请求（Certificate Signing Request）；另一个是证书服务，用来签发证书。CA 首先通过HandleCSR接口处理来自客户端的CSR请求，然后对客户端进行身份认证（包括TLS认证和JWT认证），认证成功后会调用CreateCertificate进行证书签发。
安全发现服务器（SDS） SDS是一种在运行时动态获取证书私钥的API，Istio中的SDS服务器负责证书管理，并实现了安全配置的自动化。
证书密钥控制器 证书密钥控制器可以监听istio.io/key-and-cert类型的Secret资源，还会周期性地检查证书是否过期，并更新证书。
证书轮换 Istio通过一个轮换器（Rotator）自动检查自签名的根证书，并在证书即将过期时进行更新。它本质上是一个协程（Goroutine），在后台轮询中实现。
秘钥和证书的轮换过程 Envoy通过SDS API发送证书和密钥请求。 istio-agent作为Envoy的代理，创建一个私钥和证书签名请求（CSR），并发送给istiod。 证书签发机构验证收到CSR并生成证书。 istio-agent将私钥和从istiod中收到的证书通过SDS API发送给Envoy。 Galley Galley是整个控制平面的配置管理中心，负责配置校验、管理和分发。Galley可以使用网格配置协议（Mesh Configuration Protocol）和其他组件进行配置的交互。Galley解决了各个组件“各自为政”导致的可复用度低、缺乏统一管理、配置隔离、ACL管理等方面的问题。
MCP协议 MCP提供了一套用于配置订阅和分发的API，这些API在MCP中可以被抽象为以下模型。
source：“配置”的提供端，在Istio中，Galley即source。 sink：“配置”的消费端，在Istio中，典型的sink包括Pilot和Mixer组件。 resource：source和sink关注的资源体，就是Istio中的“配置”。 数据平面 Istio数据平面核心是以Sidecar模式运行的智能代理。Sidecar模式将数据平面核心组件部署到单独的流程或容器中，以提供隔离和封装。
数据平面的Sidecar代理可以调节和控制微服务之间所有的网络通信，每个服务Pod在启动时会伴随启动istio-init和Proxy容器。其中，istio-init容器的主要功能是初始化Pod 网络和对Pod设置iptable规则，在设置完成后自动结束。
数据平面的功能 服务发现：探测所有可用的上游或后端服务实例。 健康检测：探测上游或后端服务实例是否健康，是否准备好接收网络流量。 流量路由：将网络请求路由到正确的上游或后端服务。 负载均衡：在对上游或后端服务进行请求时，选择合适的服务实例接收请求，同时负责处理超时、断路、重试等情况。 身份认证和授权：在istio-agent与istiod的配合下，对网络请求进行身份认证、权限认证，以决定是否响应及如何响应，还可以使用mTLS或其他机制对链路进行加密等。 链路追踪：对每个请求生成详细的统计信息、日志记录和分布式追踪数据，以便操作人员能够明确调用路径并在出现问题时进行调试。 数据平面实现 Envoy：Istio默认使用的数据平面实现方案，使用C++开发，性能较高。 MOSN：由阿里巴巴公司开源，设计类似 Envoy，使用Go 语言开发，优化了过多协议支持的问题。 Linkerd：一个提供弹性云原生应用服务网格的开源项目，也是面向微服务的开源RPC代理，使用Scala开发。它的核心是一个透明代理，因此也可作为典型的数据平面实现方案。 Any Question？ 边车模式使得网络请求在每次服务访问中都增加了两跳（进入服务前被拦截&amp;amp;从服务出来后又被拦截），这会不会对整体的系统性能造成影响？</description>
    </item>
    
  </channel>
</rss>
