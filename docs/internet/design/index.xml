<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>设计相关 on 北人</title>
    <link>https://stong1994.github.io/internet/design/</link>
    <description>Recent content in 设计相关 on 北人</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Mon, 01 Feb 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://stong1994.github.io/internet/design/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>理解TCP握手和挥手</title>
      <link>https://stong1994.github.io/internet/network/tcp_connection/</link>
      <pubDate>Sun, 27 Mar 2022 21:19:00 +0800</pubDate>
      
      <guid>https://stong1994.github.io/internet/network/tcp_connection/</guid>
      <description>TCP三次握手和四次挥手是面试的经典问题，网上这方面的资料繁多但往往局限在表面，只有全面了解TCP协议才能做到知其然且知其所以然。
TCP协议位于传输层，介于应用层和网络层中间，负责提供可靠的全双工的连接服务。
TCP协议的可靠性体现在多个方面，如ACK机制、强制维持校验和、重传机制等等。今天我们从可靠性的角度来看握手和挥手的过程。
ACK机制 ACK机制往往用来确保数据被正常读取或者消费，比如说Kafka、Rabbitmq提供了ACK机制确保数据被正常消费。
在TCP协议中也是用ACK机制来确保数据被正常接收，也就是当接收数据的一方获得数据后，会向发送方发送一条ACK消息表示自己接收到了这条消息。于是我们需要一个消息标识，并且需要这个消息标识在整个通信过程中是唯一的。
唯一标识 在存储服务中使用的唯一标识主要有两类：自增整数、随机字符串。TCP协议使用一个自增整数来作为消息的唯一标识，这样有以下几个优点：
 自增整数要比随机字符串更节省空间。 能够进行批量ACK，即对一段连续的消息回复最大的自增值即可表示这批消息都被正常接收。 能够检查是否有数据包遗漏。  自增规则 自增值并不是从0开始，而是在开始连接时初始化的一个随机值，这是因为如果将自增值初始化为一个固定值，那么通信过程容易被预测并攻击。
自增值也不是每条新消息都会加1，而是加上消息体（排除掉TCP头部的数据）的字节大小，这样做是因为每条消息可能很大，也可能很小，使用消息体的字节大小能够更好的表示当前通信的累计大小，也能更好的控制发送频率。
对于通信中的双方，发送方需要提供当前发送消息的序列号（这个序列号就是自增值），接收方需要将这个序列号加上消息体的字节数作为ACK号（也是发送方下一个消息使用的序列号）返回给发送方表示自己已经收到了这么多的数据。
ACK消息的可靠性 TCP协议通过ACK机制实现了通信确认的可靠性，但是ACK消息本身没有确认机制——发送消息的一方在接收到ACK消息后不会再次发送ACK消息来告知接收方自己收到了这条ACK消息，否则就会导致死循环。
但是我们也不需要保证ACK消息能被正常接收，因为ACK消息存在的意义就是保证用户的消息能够被接收方接收，因此如果发送方没有收到这条消息的ACK，那么就重新发送就好了。这就涉及到了TCP协议中的超时重传机制。
全双工 TCP协议是全双工的，即能够进行两个方向的数据发送，并且两个方向的数据发送是彼此独立的。
握手 作用 握手是通信前的一个准备过程，主要有以下几方面的作用：
 在双发通信前需要确认双方能够正常通信。 传递通信所需的初始化信息，如序列号、窗口大小、MSS等  三次握手 第一次握手，客户端需要向服务端发起连接，主要用来告知对方以下消息：
 客户端想要连接的端口号，即服务端的端口号 客户端监听的端口号：如果服务端要联系客户端就要指定为这个端口 客户端初始化的序列号、窗口大小等  在这个TCP消息中，需要在头部标记SYN标识表示这条消息为第一次握手。
第二次握手，出于ACK机制的考虑，服务端需要回复客户端，但是既然服务端也要发送自己的初始化信息，那么在ACK消息中也会携带这些信息。
在ACK机制中规定了ACK消息需要返回ACK号，而ACK号的值是序列号加上消息体的字节数的结果，但是此时消息中消息体的字节数为0，但是在第二次握手时，ACK号的值为序列号加1的结果（思考为什么一定要加1？）。
在这个TCP消息中，需要在头部标记SYN和ACK标识。
第三次握手，处于ACK机制的考虑，客户端需要回复服务端，这就是所谓的第三次握手。
由此可见，考虑到TCP协议全双工的特性和ACK机制，双方本来需要四次握手——两次发送初始化信息+两次ACK。但是TCP进行了优化，将中间的两次握手合并，最终形成了三次握手。
四次握手？ 既然三次握手是四次握手优化后的结果，那么有没有可能出现四次握手呢？
在极端情况下是可以出现的：通信双方同时发起SYN消息，这样就就没办法将一方的SYN消息和ACK消息合并，因此就会出现四次握手的情况。但是一方面很少有双方主动连接对方的场景，另一方面，这需要双方同时发起SYN消息，所以出现这种情况还是很难的。
挥手 作为可靠的传输协议，使用TCP协议连接的双方不能简单粗暴的直接关闭连接（当然有这种场景，等下再吐槽），否则可能会导致数据丢失。
四次挥手 假设是客户端发起的断开连接请求
第一次挥手，客户端要告知服务端自己需要关闭连接了。此时在TCP头部标记为FIN（实际为FIN&amp;amp;ACK，因为除了第一次握手外，其他时候的通信都要有ACK标记）。
第二次挥手，即服务端对客户端单纯的ACK回复。
第三次挥手，服务端等待对客户端的数据发送完后发起关闭请求，内容同“第一次挥手”。
第四次挥手，客户端对服务端单纯的ACK回复。
由此可见，四次挥手就是处于全双工特性加上ACK机制的两次关闭请求+两次ACK。
既然中间的两次挥手都是服务端向客户端发送消息，那能不能合二为一？
三次挥手？ 中间的两次挥手都是服务端向客户端发送消息，并且是有可能合并为一个消息的。如果服务端本来就要发送FIN包，这时候收到了客户端发来的FIN包，那么就可能会将FIN包和ACK包合并在一起。
经典问题 客户端在最后一次挥手后，为什么要等待2MSL 正常情况下，客户端在最后一次挥手后是不会再接收到服务端的消息的，因此也就不能确定服务端是否正常接收了最后一个ACK消息。
假设客户端在最后一次挥手后没有等待2MSL，并且服务端没有接收到最后一个ACK消息:
第一种情况：客户端使用这个端口重新向这个服务端发起了连接请求，此时对于服务端来说仍处于LAST_ACK状态（第三次挥手后），因此会对新的客户端发送RST包中断连接。
第二种情况：客户端复用这个端口向其他服务端建立了连接， 由于超时重试机制，服务端就会再次向客户端发送FIN包（第三次挥手），那么新的客户端接收到这个FIN包后，就可能会造成数据冲突。
因此客户端需要等待服务端超时重试的包送达的最大时间后才能关闭连接，这个时间就是2MSL。
如果在2MSL期间端口不可用，那么如果是服务端主动关闭连接（比如重启），为什么就可以重用端口 服务端在启动时，往往会激活SO_REUSEADDR，即允许端口重用。
握手次数能不能减少到两次 三次握手前两次肯定要存在，那么就考虑能不能省略第三次握手。
第三次握手本质上就是由于ACK机制引起的消息确认，从这个角度思考，问题就变成了能不能去掉ACK机制，那么答案也就很明了了：不能，因为ACK机制是TCP协议可靠性的重要保障。</description>
    </item>
    
    <item>
      <title>Redis事务</title>
      <link>https://stong1994.github.io/internet/design/redis_transaction/</link>
      <pubDate>Thu, 24 Mar 2022 13:48:00 +0800</pubDate>
      
      <guid>https://stong1994.github.io/internet/design/redis_transaction/</guid>
      <description>作为存储服务，业内往往用事务来检验其可靠性和安全性。在Redis中，事务表现为隔离性、持久性、弱原子性，最终的效果就是弱一致性（区别于BASE中的最终一致性，Redis的弱一致性就是表示一致性很弱）。
事务相关的命令有：MULTI、EXEC、WATCH、DISCARD、UNWATCH
弱原子性 服务端会将客户端提供的事务内的命令放入事务队列中，从这个队列中执行命令时会一直阻塞其他连接的命令，直到队列中的命令执行完。
对Redis来说，事务不能回滚。理由是如果要实现回滚，需要更复杂的技术实现，违反了Redis简单高效的原则。
如果仿照MySQL来实现Redis的回滚，那么每个事务在执行时都要保存一份回滚日志，每次执行事务中的写命令时都需要写对应的回滚日志，事务执行结束后清除回滚日志（因为是单线程执行命令，回滚要比MySQL简单很多）。
事务中的命令在执行前，会先检查语法（入队时检查），如果语法错误，那么所有的命令都不会执行。如果语法没问题，但是执行时出现错误，那么事务仍会执行接下来的命令。
隔离性 Redis使用单线程来处理命令，并且保证执行事务队列时不会中断去执行其他连接的命令，因此具有隔离性。
持久性 Redis有两种持久化机制: RDB和AOF。RDB机制缺乏实时存储，但是AOF机制能够保证命令执行后同步到硬盘（appendsync选项设置为always时）。
此外，在执行EXEC命令前执行SAVE命令能够保证事务的持久性，但是效率很低。
Pipeline与事务的区别 参考资料
 执行事务时会阻塞其他命令执行，而pipeline不会 pipeline会将命令一次性打包发送到服务单，而事务会一条一条发送 pipeline是客户端的行为，是将多条命令打包到一起发送给服务端。服务端解析这些命令然后执行，并不清楚是否是pipeline。如果这些命令数据较少，能够一次性的写入服务端的输入缓冲区，那么这些命令的执行就不会被打断，但是如果这些命令数据较大，那么这些命令就可能会被分成多次发送给服务端，命令的执行就可能会被打断  </description>
    </item>
    
    <item>
      <title>Redis缓存设计</title>
      <link>https://stong1994.github.io/internet/design/redis_cache_design/</link>
      <pubDate>Thu, 24 Mar 2022 13:48:00 +0800</pubDate>
      
      <guid>https://stong1994.github.io/internet/design/redis_cache_design/</guid>
      <description>Redis最常用的场景就是作为缓存。
缓存带来的收益就是加速读写，降低下游存储的压力。
但引入缓存的同时也增加了一些成本与潜在问题。
缓存穿透 当客户端访问一个即不存在于缓存层，又不存在于存储层的数据时，为了保持“数据一致性”，服务端会直接将空结果返回。但是缓存这样就失去了保护存储层负载的意义——如果有大量这样的恶意攻击，存储层会由于请求太多导致响应慢，牵一发而动全身，整体系统都会受影响甚至崩溃。
缓存穿透有两种解决办法：缓存空对象和使用布隆过滤器。
缓存空对象 通过在缓存层保存一个NULL值就能保护存储层免于袭击。
但这同样有缺点：
 缓存中可能存在大量的NULL数据，会占用大量宝贵的内存。 缓存层和存储层数据不一致：存储层可能会写入在缓存中为NULL的数据。  可通过对NULL数据设置较短的缓存时间、使用合理的缓存清理方案来缓解上面两个问题。
使用布隆过滤器拦截 可以使用布隆过滤器来拦截掉不存在的数据请求。这种方案的缺点就是代码复杂度会更高。
缓存击穿 “击穿”和“穿透”两个词的相似性太高，往往使人迷惑。所以我们往往使用热点key问题来描述。
一个热点数据往往有着大量的并发请求，我们要小心处理这些热点数据，否则一旦缓存失效，巨量请求会直接使存储层响应变慢甚至崩溃。
缓存击穿的场景往往是缓存失效导致的，解决方案有：通过加锁限制存储层的访问数量、设置“随机”的过期时间避免大量数据一起失效、设置缓存永不过期等
通过加锁限制存储层的访问数量 当缓存失效后，使用全局锁来实现只允许一个线程请求存储层，其他线程等待这个请求的结果。
这种方案的缺点是代码实现更复杂，并且如果获取到锁的线程访问有异常，会导致大量的请求超时。此外，还会有死锁这种潜在问题。
设置“随机”的过期时间 设置“随机”的过期时间是为了避免大量数据一起失效，这样能够分批请求存储层，减少存储层压力。
但是如果有一个超热数据，仍会对存储层造成压力。
设置缓存永不过期 设置缓存永不过期能够避免缓存失效问题。但是需要在代码上增加复杂度——判断何时对缓存进行更新、删除。
缓存雪崩 缓存雪崩是指缓存服务器异常，缓存全部失效，导致存储层压力骤增。
这种时候可以先提高缓存层的可用性，如使用哨兵模式或者集群模式。然后再进行其他优化，如：限制请求频率。
除了对缓存层进行优化外，还要从整体角度来考虑，比如增加降级机制来避免整体系统崩溃。
无底洞问题 无底洞问题是指在一个分布式缓存集群中，添加节点并没有加快请求，反而使请求更慢。这一问题往往是发生在批量获取数据时产生的。
由于数据分布在多个节点中，因此一个批量操作会涉及到多次网络操作，另外，网络连接数变多也会影响服务器性能。
我们假设需要执行mget命令批量读取多个数据，有以下几种方案：
串行命令 最简单的方式就是有几个key就进行多个次get请求，但是这种方式无疑也是性能最差的。
客户端聚合key 客户端能够提前在本地缓存key-&amp;gt;槽-&amp;gt;节点的映射关系，因此可以先遍历key，将在同一节点的key执行批量操作，这样能够减少网络请求。
并行IO 在上一步的基础上，通过异步请求将串行IO变为多线程，能够进一步加速请求。
hash tag Redis集群提供了hash_tag功能，将多个key强制分配到一个节点上。但是这种方式需要更高的维护成本，还容易形成数据倾斜。</description>
    </item>
    
    <item>
      <title>Redis复制、哨兵、集群</title>
      <link>https://stong1994.github.io/internet/design/redis_multi_server/</link>
      <pubDate>Thu, 24 Mar 2022 11:33:00 +0800</pubDate>
      
      <guid>https://stong1994.github.io/internet/design/redis_multi_server/</guid>
      <description>随着访问量增多，我们常常对存储服务进行读写分离来降低主服务器的压力，读写分离最常用的方式就是增加从服务器。从服务器复制主服务器的数据，并提供给外部处理读请求。
主从复制 同步 当客户端向从服务器发送SLAVEOF命令，要求从服务器复制主服务器时，从服务器会先执行同步操作。
 从服务器向主服务器发送SYNC命令。 收到SYNC命令的主服务器执行BGSAVE命令，在后台生成一个RDB文件，并将新产生的命令放入到一个缓冲区中 BGSAVE命令执行完后，主服务器将生成的RDB文件发往从服务器，从服务器载入RDB文件。 从服务器载入RDB文件完毕后，主服务器将缓冲区中的写命令发送给从服务器。  同步完成后，如果主服务器再次接收到新的写命令，那么主服务器会将命令发送给从服务器，来保持数据一致。
复制积压缓冲区 主服务器在执行完命令后将该命令传播到从服务器，如果这时从服务器发出故障或者网络波动导致命令未在从服务器执行，那么主从之间的数据就会不一致。
为了判断主从之间的数据是否一致需要引入复制偏移量。每当主服务器发送N个字节或者从服务器接收N个字节的数据时，就将复制偏移量加上N。
如果主从之间的复制偏移量不同，那么就需要进行同步。如果只丢失了一小部分数据，那么没必要进行完整的数据同步，所以需要一个结构来存储最近的命令，这个结构就是复制积压缓冲区。
复制积压缓冲区是一个FIFO队列，当主服务器执行完命令后，就会将该命令写入到复制积压缓冲区，然后再将该命令传播到从服务器。
当发现主从服务器之间的复制偏移量不同时（通过ping或者从服务器重启），主服务器会判断从服务器的复制偏移量后的数据是否还在复制积压缓冲区内，如果在，就直接将复制偏移量后的数据发送给从服务器，否则，进行完整的数据同步。
哨兵模式 在主从复制的模式，一旦主服务器发生故障，从服务器并不会主动选举出新的master，需要运维手动设置master，这势必会造成一段时间内的服务不可用。为了提高可用性，Redis提供了哨兵来监控服务器。
服务发现 哨兵会定期发送INFO命令到其监控的服务器中，主服务器会将其角色和从服务器地址返回给哨兵，因此哨兵只要监控主服务器就能获得从服务器的地址。
哨兵会监听同一个频道信息，也会向这个频道报告自己的信息，因此哨兵之间都能够发现彼此。
选举领头哨兵 当主服务器发生故障后，需要领头哨兵进行故障转移，Redis通过raft算法实现了选举功能。
 发起选举后，每个哨兵在每个配置纪元里都能够设置自己认可的leader，一旦确认，在这个配置纪元里就不能再修改 每次选举，配置纪元都会自增 “认可”leader的规则依据先来先得，即先接收到的认可请求会被接受，后接受的会被拒绝 选举规则采用多数服从少数，一个哨兵只要被半数以上的哨兵认可就会被选举为leader 如果在给定期限内没有选举出leader，那么会再次进行选举  主服务器故障确认 哨兵会定期向其监控的服务器发送PING命令，如果主服务器在一段时间内没有回复，那么哨兵就会认为主服务器故障。
但是每个哨兵配置的超时时间可以是不同的，因此这个哨兵会向其他哨兵确认主服务器是否故障，当超过quorum数量的哨兵认为主服务器已发生故障，那么就可以认为主服务器发生了故障，需要进行转移。需要注意每个哨兵的quorum可以是不同的。
故障转移 故障转移需要先在从服务器中选举出主服务器。
 排除掉已经下线的从服务器 排除掉与哨兵存在通信故障的从服务器 选择出数据最新的从服务器（根据与旧的主服务器断开时长来判断） 在剩余的从服务器中选择优先级比较高的从服务器 在剩余的从服务器中选择复制偏移量最大的从服务器 在剩余的从服务器中选择id排序最小的从服务器  选出主服务器后，哨兵会向候选服务器发送slaveof no one明确将其“提升”为主服务器。然后向其他从服务器发送命令修改复制目标为新主服务器。
如果旧的主服务器上线，上线后会成为从服务器。
集群 随着数据量不断膨胀，分布式存储变得日趋重要。Redis集群中舍弃了“哨兵”这类管理者，使用分片进行主节点之间的数据切分，使用Gossip协议实现了各个主节点之间的信息共享。
分片 Redis集群通过分片实现了主节点之间的数据分配。整个集群就是一个数据库，数据库被分为了16384个槽，需要手动分配这些槽到指定节点上。
每个节点都通过长度为16384的二进制数组来标记该节点负责哪些槽，同时又通过另外一个长度为16384的槽来记录每个槽对应的节点信息。
分片规则是通过对键进行CRC16，并对16384取余（实际是&amp;amp;16383），结果即为目标槽，通过上边提到的数组就能获取到目标节点。
当客户端访问一个节点时，如果所需的数据不在当前节点，则当前节点会返回一个MOVE错误，同时返回数据所在的节点地址。客户端收到MOVE错误后，会重新向目标节点请求数据。如果数据所在的节点正在进行重新分片，并且目标数据已被迁移至分片后的节点，那么当前节点会返回一个ASK错误，同时返回数据所在的节点地址，客户端收到ASK错误后，会重新访问分片后的节点请求数据。
更智能的客户端 客户端可以自己维护键-&amp;gt;槽-&amp;gt;节点的映射关系，这样就不需要每次都“猜”目标节点是哪个。
节点信息共享：Gossip 两个节点之间通过“三次握手”进行连接，连接之后，将彼此的信息通过Gossip协议扩散到其他节点，这些信息包括：
 节点自身数据，包括分片后的槽的分配信息 整个集群1/10的节点的状态数据  集群的节点间会定期发送PING消息来检测对方是否在线，如果每个节点都向所有节点发送消息那么会凭空增大服务器压力，因此对于每个节点，先随机从节点列表中选出5个节点，然后从这5个节点中获取最长时间没有发送PING消息的节点发送PING消息。此外，每100毫秒节点都会遍历自己的节点列表，找到超过某段时间内没有通信的节点，然后将其加入到发送名单中。
故障转移 集群中的每个节点都会定期向其他节点发送PING消息来检测对方是否在线，如果对方没有及时回复则会被视为疑似下线，节点之间会分享彼此的信息，当集群中半数以上的主节点都认为该节点已下线时，那么这个节点会被标记为已下线，将该节点标记为已下线的主节点会在集群中广播一条FAIL消息，收到FAIL消息的主节点会立即将该节点标记为已下线。
选举主节点 集群会从已下线的主节点的所有从节点中选举出新的主节点：
 对从节点进行资格筛选：如果从节点与下线的主节点的最后通信的时间间隔超过一个阈值，那么这个从节点就失去了选举资格（如果所有的从节点都失去了资格，就需要手动进行强制转移） 设置选举优先级：通过对比从节点的复制偏移量来获得其优先级，复制偏移量越大的从节点的优先级越高，对这些从节点进行优先级排序，优先级低的节点的选举发起时间会比前一个优先级更高的节点的选举发起时间晚1秒。 广播选举消息：每个有资格发起选举的从节点根据自己的选举发起时间进行消息广播。 每个有投票权的主节点（有负责的槽）在每个配置纪元里都有一次投票的机会，选举采用先到先得的方式，会投票给第一个收到请求的来源从节点。 当一个从节点获得了半数以上的选票时，会被“升级”为主节点。落选的从节点修改复制目标为新的主节点，当旧的主节点上线时也会自动成为新的主节点的从节点。 如果在一个配置纪元中没有从节点能够获得半数以上的选票，则再次进行选举。  加快故障转移时间 节点间通过Gossip协议来交流彼此的信息，包括节点的状态。但如果只通过Gossip来传播，那么下线故障节点会很慢。</description>
    </item>
    
    <item>
      <title>Redis持久化机制实现</title>
      <link>https://stong1994.github.io/internet/depth/redis_durability/</link>
      <pubDate>Mon, 21 Mar 2022 11:08:00 +0800</pubDate>
      
      <guid>https://stong1994.github.io/internet/depth/redis_durability/</guid>
      <description>Redis使用内存进行数据的读写，如果服务永不崩溃、服务器不会宕机——Redis服务永远正常运行，那么数据就不需要落盘。但目前的技术水平还不能达到这一水准，因此我们还是需要将数据存储到磁盘上，当服务重启时就可以加载这些数据。
RDB 我们可以将内存中的数据直接copy到磁盘中，这种持久化方式就是RDB。
为了防止已经copy到磁盘中的数据被用户修改，在copy过程中，Redis服务需要拒绝写操作，比如将用户请求先放入队列中，一旦copy结束再从队列中获取请求进行执行。但是这样需要考虑很多场景、条件，Redis本着简单、高效的准则，采用了最简单粗暴的方式——拒绝外部请求。
但是拒绝外部请求会导致服务不可用，这是我们不能接受的，因此在copy时Redis服务会fork出一个子进程，fork完之后，父进程就可以继续工作，由子进程来进行copy。（copy on write。。。）
AOF（Append Only File） RDB持久化最明显的缺点就是缺乏实时性，为了弥补这一点，Redis仿照文件追加的方式设计了AOF持久化——每执行一条写命令，就将该命令写到磁盘中。为了减少磁盘IO（毕竟太慢了），Redis需要先将命令写入到缓冲队列（aof_buf）中，然后再同步到磁盘中。（事件循环？）
“同步到磁盘”有三种方式：
 将缓冲队列中的数据直接写入并同步（fsync）到文件中。 将缓冲队列中的数据写入到文件中，并每隔一秒进行一次同步。 将缓冲队列中的数据写入到文件中，同步操作由操作系统控制。  AOF重写 随着命令的增多，AOF文件越来越大，为了解决AOF文件膨胀问题，Redis提供了文件重写功能。
一个key往往对应着多条命令，为了找到key对应的数据，直接读取内存会更方便。因此，AOF重写的过程就是将内存中的数据copy到AOF文件的过程。在copy过程中，为了Redis对外提供服务，因此fork出子进程来实现AOF重写，同时，写命令会存入AOF缓冲区和AOF重写缓冲区分别用于现有AOF文件的同步和AOF重写。重写完成后会将新AOF文件覆盖旧AOF文件。</description>
    </item>
    
    <item>
      <title>go设计之channel</title>
      <link>https://stong1994.github.io/internet/go/channel/</link>
      <pubDate>Thu, 10 Mar 2022 14:43:00 +0800</pubDate>
      
      <guid>https://stong1994.github.io/internet/go/channel/</guid>
      <description>channel是go中非常具有特色的设计，并且也是新手最难掌握的数据类型。
channel的设计体现出了作者“不要用共享内存来交流，用交流来共享内存（Do not communicate by sharing memory; instead, share memory by communicating）”的观点。
channel的作用 使用channel时需要配合协程，一些协程负责写数据，另外一些协程负责读数据，channel的作用就是将这两部分协程连接起来，可理解为channel是这些协程和数据的管理者。
channel的核心逻辑 读和写 作为数据的管理者，channel内部使用环形队列来存储数据，关键的数据结构为：队列、读索引、写索引：
 每写/读入一个数据，写/读索引就往右移动一位，如果索引已经是最后一位，那就移到第一位。 如果写入时队列已满，那么写数据的协程就进入等待队列，等到队列存在空位置时，再唤醒这个协程，写入数据。 如果读取时队列为空，那么读数据的协程就进入等待队列，等到队列中存在数据时，再唤醒这个协程，读取数据。  此时可以看到channel中作为协程的管理者，需要两个等待队列（读和写）来存储这些“需要等待”的协程。
关闭channel 日常工作中经常能够用到关闭操作，了解其内部逻辑能够帮助我们更好的使用它。
源码（runtime/closechan）逻辑大致如下：
 加锁 标识channel已关闭 释放所有正在等待读取的协程（channel已关闭，不再有新数据） 释放所有正在等待写入的协程（这些协程会panic） 释放锁  需要再补充一些逻辑：
 读取数据时，如果channel已标识关闭，并且队列为空，那么会标识未接收到数据 读取数据时，即使channel已标识关闭，但如果队列不为空，那么仍会进行读取 写入数据时，如果channel已标识关闭，则会panic  综上可知，关闭channel后：
 队列中的数据还是会被消费完 如果再写入数据，会panic 如果再读取数据，会得到零值，第二个返回值为false  实际工程中经常利用第三点，将关闭channel用于通知其他协程。
源码中的优化 源码在实现时，对“核心逻辑”进行了一些优化，如：
 写入数据时先判断是否存在正在等待读的协程，如果存在，直接将数据交给等待读取的协程，而不是先写入队列。因为存在正在等待读的协程，说明队列此时是空的，因此直接将数据交给这个等待读的协程不会影响数据的消费顺序，还能减少一次队列写入和一次队列读取。  思考：两个等待队列是否能同时存在 即：一个channel中是否能够同时存在正在等待读的协程和正在等待写的协程？
在上面“源码中的优化”中，我们已知：如果存在正在等待读的协程，那么写入时，会直接将数据交给这个等待读的协程，这样这个写入的协程就不会被放到等待队列中，因此结论是两个等待队列不能同时存在。
相关文章  Share Memory By Communicating 一文带你解密 Go 语言之通道 channel  </description>
    </item>
    
    <item>
      <title>简述redis的基本实现</title>
      <link>https://stong1994.github.io/internet/depth/redis_base_design/</link>
      <pubDate>Mon, 22 Nov 2021 16:05:00 +0800</pubDate>
      
      <guid>https://stong1994.github.io/internet/depth/redis_base_design/</guid>
      <description>很久之前就看过redis的基本设计与实现，但是每次都会忘掉。
前几天又看了一遍，但是最近回顾的时候又忘了。。。
俗话说好记性不如烂笔头，因此写在这里来加深记忆。
文中会将数据类型的实现与go中的实现进行对比，如有理解错误的地方，望指出
五个基本数据类型 string  go中的string：在go中，string就是一组字节，且是不可变的。可以视作字节数组。
 redis中的字符串对象的编码可以是int、raw或embstr。
如果保存的对象是整数且可以用long类型来表示，那么就保存为整数，编码为int。
如果保存的对象是字符串且长度小于等于32字节，那么会使用embstr的编码来保存。
如果保存的对象是字符串且长度大于32字节，那么会使用embstr的编码来保存，且存储在SDS中。
embstr是专门用来保存短字符串的一种优化编码方式，与raw的区别在于对于redisObject和sdshdr（redisObject是redis对象中的一个属性，sdshdr是SDS的实现），embstr只需一次内存分配，而raw需要两次。
SDS 简单动态字符串（SDS）组成：
 buf: 字节数组 len: 字符串长度（即buf数组中已使用的字节数量） free: buf数组中未使用的字节数量  SDS遵循C字符串以空字符结尾的惯例，保存空字符串的1字节空间不计算在SDS的属性中。
空间预分配策略：修改之后的SDS长度小于1M，那么程序会分配同样大小的预留空间，即len=free；如果修改之后的SDS长度大于1M，那么程序会分配1M的预留空间。
空间惰性释放策略：SDS中的字符串长度减小时，并不直接释放空间，而是增大free，可供未来使用，避免频繁释放/分配空间。
list  go中的slice
构成：由三个属性构成：长度、容量、底层数组。
扩容策略：当容量小于1024时，每次扩容为原来容量的一倍；否则扩容1/4
缩容策略：无
 当list中元素的字符串长度都小于64字节且元素数量小于512时，使用压缩列表实现，否则使用双端链表实现。
双端链表 双端链表有如下几个属性：
 表头节点 表尾结点 节点长度 节点复制函数 节点释放函数 节点值对比函数  节点有如下属性：
  前置节点地址
  后置节点地址
  节点值
  压缩列表 压缩列表包含的属性：
 整个压缩列表占用的字节数 计算列表尾结点距离压缩列表的起始地址有多少字节 记录压缩列表包含的节点的数量（当总数大于65535时，这个字段失效，需要遍历整个压缩列表才能计算出来） 列表节点数组（每个节点可以保存为一个字节数组或者整数）  列表节点包含的属性：
 上一个节点的长度 编码类型与长度 节点内容  压缩列表的优点就是节省内存，缺点就是增加、删除、更新可能会造成“连锁更新”，因此只有在包含少量元素时才使用。</description>
    </item>
    
    <item>
      <title>以真实经历谈分层</title>
      <link>https://stong1994.github.io/internet/depth/layer/</link>
      <pubDate>Sat, 20 Nov 2021 13:20:00 +0800</pubDate>
      
      <guid>https://stong1994.github.io/internet/depth/layer/</guid>
      <description>笔者在工作过程中遇到了一些分层相关的问题，于是将问题和想法记录下来，以供未来回顾。
提出问题  什么是分层 为什么要分层 怎样做分层  什么是分层 这是一个很简单的问题。
这也是一个很复杂的问题。
简单之处在于每个人都能做出回答，复杂之处在于这其实是个通用问题。
通用问题是啥？百度百科上是没有这个词条的，因为我不知道这类问题如何划分，所以随便造了个词，或者称为底层逻辑问题更好理解些？
程序员当然知道有哪些分层：网络有分层、操作系统有分层、项目有分层、代码有分层等等。
但生活中的分层要更多。
 每天早上吃的鸡蛋有分层：蛋壳、蛋白、蛋黄 上班路上两边的树木有分层：树根、树干、树冠、树叶，或者将其拦腰斩断，能看到层次分明的年轮 坐电梯时可能更能体会到分层——每层楼都是一层。 进入公司，销售部、行政部、研发部等等也在分层 连我们人体本身也满是分层：上半身、下半身、头、胳膊、脚，或者皮肤、脂肪、血液、骨骼、神经等等  分层是这个世界的基本规则之一。
思维的发散就到此为止吧，因为我已经发现没有办法直面我们最初的三个问题了。
所以让我们来简化下问题——将问题的讨论范围限制在代码内。
对于什么是分层——我先给出我的答案——分层就是对代码按照某种规则进行切分。
至于为什么是这个答案，下面会讲。
为什么要分层 我们先来回顾下分层的演进。
最早的分层是什么呢，那一定是没有分层。当我们打印出“hello world&amp;quot;时，我们是没有分层的。
让我们继续写代码。我可能要在前端展示一些文字，这些文字可能存储在数据库中。如果仅仅是这样的话，我们很可能还是没有分层——功能实在是太简单了。
直到有一天，我们写了上千行的代码，突然发现代码已经很难维护了，因为数据模型、业务逻辑、前端代码等都混在一起，于是我们本能的开始分层。于是一个伟大的概念产生了——MVC。
MVC最早据说是起源于桌面端开发，M代表数据层，V代表UI层，C代码控制层，通过分离这三层，我们的代码已经是很清晰了。
但是该死的产品经理还在没完没了的增加那些不知道有什么用的功能。
于是代码开发者发现三层不够用，于是把前端和后端代码进行了隔离，也就是前后端分离。后端将已有的两层扩展为三层——控制层-逻辑层-数据层（controller-service-model）。那么前端呢？前端都分出去了，我们就不管了。
 这里有个逻辑要叙述下。有些人认为是ajax这类技术的产生才导致了前后端分离。这种想法属实是本末倒置了，任何技术的产生都来源于需求！
 我对于controller-service-model这种分层可谓是异常熟悉，因为就在我大学实习的时候，就用的这种分层。当时用得是java的SSM框架，三个框架正好对应这三层（java好像搞啥都是一整套？）。这几个框架让我深受贫血模型的影响，即使我后来不写Java了。
时代在发展，软件的用户越来越多，功能越来越复杂，开发人员越来越多。代码也越来越臃肿。
于是某个大佬发明了微服务的概念，再然后某个大佬发明了中台的概念。
于是我们不仅有前后端的分层，还有后端与后端的分层——前台、中台、支撑的分层。
回到我们的问题——为什么要分层——答案应该已经很明确了，就是为了解决代码的臃肿问题，让代码更清晰！
怎样做分层 服务分层  现状：目前公司内有很多中台仅仅是对数据库的CRUD的封装（看起来就像是封装了一个使用http做传输的ORM框架），业务逻辑仍集中在前台。这种中台没有任何意义，似乎只是为了分层而分层，或者说为了做中台而分层。进一步的原因就是设计者对中台缺乏认知。
 目前我们的项目存在两种分层方式：按功能划分与按业务划分。
以报表功能举例：我们在多种场景中都需要报表功能，如人事报表、招聘报表。这些报表都有自己的业务逻辑，不能进行统一处理，但是都需要订阅功能，且都存在业务逻辑：当用户删除报表时，需要同时删除用户对该报表的订阅（该功能在下文用功能A标识）。
按功能划分 根据功能的性质划分，此时订阅功能和报表功能为同等级功能。
此时会存在：报表中台、报表前台、订阅中台、订阅前台。
功能A应在报表前台来实现。
按业务划分 按照业务来划分，此时订阅功能应被视为报表的附属功能。
此时会存在：招聘报表中台、招聘报表前台、人事报表中台、人事报表前台。
功能A应在招聘报表中台和人事报表中台分别实现。
划分手段 上边直接说了结论，那么这样划分的依据是什么？
首先必须要分为中台和前台：中台作为业务的聚合，而前台作为对前端的适配。这样能保证业务逻辑的内聚，使中台专注于自己的业务，避免易变的产品需求对业务核心代码的侵蚀。
其次一定要让服务有明确的边界。设计者不能凭感觉来划分服务，一定要有自己的方法论作为指导基础。如果只凭感觉来划分，最终的结果就是服务之间没有边界，导致中台服务冗余了大量不属于自己领域内的代码。
所以不管是按功能划分还是按业务划分又或者有其他划分方法，总之设计者一定要有自己的划分方法论。
代码分层  现状：目前公司内大量项目的代码结构为controller+business+service。business做业务逻辑，service做服务实现。换句话说，就是将以前的service层改名为business，以前的model改名为service。这种改变的逻辑是：微服务时代需要大量调用其他服务，model不具有此含义，因此需要将model改名为service，用service来处理调用其他服务的逻辑。
这种结构在实际开发中面临一个非常严重的问题——business和service的边界模糊——导致service层的代码和business层代码混在一起——导致本就复杂的业务层代码更加复杂且难以理解。
 如何解决business和service的边界模糊问题 边界模糊的原因1：词汇描述能力不足。我们一般使用service来写业务逻辑，现在换用了business，但是仍保留service层来做服务调用，这增加了开发者对service和business语义上的模糊。另外，从读者的角度来看，这种命名会让人十分疑惑。</description>
    </item>
    
    <item>
      <title>谈代码规范、思想</title>
      <link>https://stong1994.github.io/internet/depth/code_thought/</link>
      <pubDate>Wed, 06 Oct 2021 17:05:00 +0800</pubDate>
      
      <guid>https://stong1994.github.io/internet/depth/code_thought/</guid>
      <description>谈起代码设计规范，人们总会说出SOLID、KISS、DRY等等专业词汇。
为了易于人们记忆，这些专业词汇都是由其英文单词首字母拼接起来的，如KEEP IT SIMPLE 、STUPID、DONT TRY YOURSELF。
我们当然也能理解这些设计规范的意思——毕竟有那么多的博客、文章。
然而有多少人能真正使用这些设计规范呢？相比于知道它们的人数，实际使用过的人数应该很少。
 知易行难。知行不能统一，还是不知。
 大部分人在刚入行时写的代码都“too young too simple”。很幸运的是我在刚入行时就被一位大佬告知：如果你哪天领悟了SOLID原则，就能写出好代码。
被告知之前，我有看过SOLID原则，被告知当天，我又看了一遍SOLID原则，被告知以后的以后，我也会不时的看下SOLID原则。但是我一直都没能彻底了解这个原则——我只是看懂了那些博客说的是什么，但是究竟要如何使用SOLID原则还是一头雾水。
也许这些原则就不是用来指导人们如何使用的，而是告诉人们好的代码应该是怎样的。
程序猿也是在不断进化的。
刚入行的小伙子是“鲁莽”的，他们的眼里好像只能看到“需求”，他们会飞快地将功能实现。这时候的代码没有遵循任何的设计原则，代码混乱，很容易产生bug。而且解决这些bug需要很长的时间，因为他们的代码在实现功能时没有体现业务逻辑。理清为什么这样写要耗费人不少耐心。如果这些代码被一些有强迫症的人看到，一定会给它重构一遍。
有些工作经验的开发者会学习业内比较有名的技术、思想，就像现在的中台、DDD、TDD等。当他们照猫画虎得将这些用到实际项目中时，另外一些没学过这些技术的人会对这些东西表示怀疑——这些东西似乎让代码变得更加复杂，且没看到任何收益。
经过一段时间的怀疑人生后，对技术照猫画虎的人们开始否定这些技术，认为这些技术是徒有其表。
当开发者的经验积累到一定程度后，会开始反思当前的架构是否合理，于是尝试对其进行改进。然后就会突然发现，他竟然运用到了这些设计原则、思想。
那么以后会怎样呢？</description>
    </item>
    
    <item>
      <title>异地多活</title>
      <link>https://stong1994.github.io/internet/design/dboat/</link>
      <pubDate>Mon, 04 Oct 2021 22:32:00 +0800</pubDate>
      
      <guid>https://stong1994.github.io/internet/design/dboat/</guid>
      <description>背景 随着用户的日益增多，系统的质量问题越来越突出。
想象一下：用户正在使用软件，突然软件崩溃了、不能用了，这时候用户肯定要理(ma)解(niang)的。如果一年只崩溃一两次还好（当然，如飞机、火车运行所需要的软件是绝对不能出问题的），如果每隔几天就来这么一下，那么用户可能就要寻找替代品了。2B的产品更是如此（业内通常使用SLA来描述可靠性，也就是大佬们常说的4个9、5个9）。
提升服务质量的手段有很多，如：
  良好的代码风格、积极的code review、完善的自动化测试——在根上减少问题出现的可能性
  合理的监控、报警、预警——保证第一时间内得到通知甚至提前预知风险
  科学的熔断策略——减小一个低质量的服务造成全体系统崩溃的风险
  完善的链路追踪、日志系统——提高解决问题的速度
  善用灰度网关——减少重构系统带来的风险以及损失
  。。。
  尽管目前的手段众多，但是如果一个地区发生了“黑天鹅”事件，如没有预警的停电、地震、海啸，又碰巧这就是我们的服务器所在地，那么上述手段也是无能为力。
所以我们就需要更强大的容灾方案——异地多活。
目标 实现两地三中心方案。
什么是两地三中心？就是在两个区域部署三套服务——一个区域一套，另外一个区域两套。大部分两地三中心是在同城双活的基础上，增加了异地灾备数据中心。而对我们来说，其实就是实现的多区域同步设计方案，只是在实施上是两地三中心。
为什么不是三地三中心？因为城市之间要通过光缆来传输数据，而这是一笔很大的开销。
功能列表：
 用户“就近访问” 区域之间的数据同步 一个区域的服务器宕机后，流量自动打到其他区域 等  仅看这个功能列表，很多细节都很模糊（不是模糊，是根本就没有），我们先看设计方案，然后再把剩余的细节问题解决。
设计方案 两区域间单向的数据流 上图是区域之间数据的单向流动。
 数据库的同步组件选择了阿里开源的canal，它会模拟从服务器来获取数据库的binlog canal支持tcp、kafka、rocketmq三种同步方式，我们选择kafka 发送端：主动发起同步的区域从kafka中获取到数据，然后发往被同步的区域 接收端：被同步的区域接收数据的服务即为接收端，接收到数据后会放到kafka中。这里kafka的作用是削峰与暂时的持久化。 回放端：从第四步中的kafka中获取数据，解析为sql，并执行，完成数据的回放  以上步骤解决了两个区域之间的单向同步
两区域间双向的数据流 跟前一张图相比，只是进行了“镜像复制”，逻辑没有增加。
但是我们发现了数据回环——即从A区域的数据同步到B区域之后，又回到了A区域。如何打断数据回环？
一般来说，我们以“就近原则”为准，能在B区域打断就不要在A区域打断，这样至少能减少数据传输。
我们能控制得只有接收端、回放端和发送端，并且需要在入库之前打上标记，入库拿到数据之后进行过滤。根据“就近原则”，我们在回放端标记数据，在发送端进行数据过滤。具体方案如下：
将数据信息记录到redis的hash中，key为`replay:{数据库名}:{表名}`， field和value规则如下： 1. 对于DDL, field为crc32(sql)+区域标识, value为serverID 2. 对于插入, field为操作类型标识+主键ID+区域标识， value为来源serverID 3. 对于删除, field为操作类型标识+主键ID+区域标识， value为来源serverID 4. 对于更新, field为操作类型标识+主键ID+crc32(after)+区域标识, value为来源serverID  其中serverID为数据库实例的唯一标识，这里只来源实例。 after为更新后的列数据，在实现中是一个结构体。插入和删除都是幂等的，因此不需要记录列信息，更新操作需要判断是否为同一条语句只用主键是不行的，所以需要记录列信息。  发送端从kafka获取到数据后，先判断数据是否是回环数据，如果是则过滤，然后删除缓存。</description>
    </item>
    
    <item>
      <title>灰度网关</title>
      <link>https://stong1994.github.io/internet/design/gray_gateway/</link>
      <pubDate>Sun, 03 Oct 2021 23:57:00 +0800</pubDate>
      
      <guid>https://stong1994.github.io/internet/design/gray_gateway/</guid>
      <description>背景 对于一个公司来说，在创业初期需要对产品快速迭代来解决用户痛点、提升自己的竞争力进而占领更多的市场（这就是MVP原则的思想）。随着业务的发展，早期的快速迭代导致了代码冗余、混乱、质量低、难以维护等问题，这时候就需要对其进行重构。
但是重构会带来极大的风险，严重的会导致服务崩溃，甚至是数据混乱。这是我们不能接受的。
尽管重构的风险是无法避免的，但是我们却可以通过管控流量将风险降到最低。这就用到了灰度网关。
此外，灰度网关还支持A/B测试等其他方面的功能。
功能简介  支持将旧服务的流量打到新服务 支持按照一定的比例来分配流量 支持按照header或者ip来分配流量 支持动态配置  正常情况下的流量走向 灰度后的流量走向 技术选型 开发网关，那首选就是openresty了。openresty是一个以nginx作为网络通信，以lua语言进行开发的平台，也可以理解为是一套可以通过lua语言对nginx进行扩展的生态。
由于需要支持动态配置，因此需要一个配置中心，我们选择了consul（整体系统的配置中心都是用的consul）。
开发思路 nginx执行阶段选择 先来回顾下nginx的11个执行阶段。
openResty的11个*_by_lua指令，对应了nginx的11个执行阶段。以下是其应用场景：
 init: 只会在 Master 进程被创建时执行 init_worker: 只会在每个 Worker 进程被创建时执行 ssl_certificate: 证书认证 set: 设置变量 rewrite: 转发、重定向 access：权限控制 content：生成内容返回 balancer：负载均衡 header_filter: 响应头过滤 body_filter: 响应体过滤 log: 日志记录  通过上图，我们可以得出结论：我们只能在set、rewrite、access这三个阶段进行灰度处理
判断流量走向 首先，如果url没在配置中，那么流量一定是打入到原环境。
如果url在配置中，那么流量需要按照比例判断是否打入到灰度环境还是原环境。
判断url是否在配置：
 通过ngx.var.uri即可拿到访问url，然后再去配置中心进行匹配即可。  判断该请求打入到哪个环境：
  在头部拿到token：ngx.req.get_headers()
  如果token为空获取ip：
local headers = ngx.req.get_headers() local ip = headers[&amp;#34;X-REAL-IP&amp;#34;] if ip == nil then ip = ngx.</description>
    </item>
    
    <item>
      <title>事件分发平台</title>
      <link>https://stong1994.github.io/internet/design/evps/</link>
      <pubDate>Fri, 01 Oct 2021 19:26:00 +0800</pubDate>
      
      <guid>https://stong1994.github.io/internet/design/evps/</guid>
      <description>背景 随着业务的增长，一个事件开始被多个子系统订阅，如用户注册事件就可能被处理用户逻辑的子系统和日志系统订阅。以往我们在处理这些逻辑的时候，要么在处理完注册逻辑后，调用多个子服务接口，要么用消息队列中间件来处理。这些都导致了较高的维护成本。
另外，整体系统使用了严格的分层，下层服务不能调用上层服务，同层之间也不能调用。如果有回调等需求，也要通过事件的方式来传递数据。
因此，我们基于消息队列的思想，创建了事件分发平台。
架构设计 整个事件分发系统大致由以下构成：
 事件发送方：即事件的生产者 事件订阅方：即事件的消费者 事件分发服务：接收事件，处理事件的发送逻辑 事件管理平台：通过web页面管理事件的配置，显示错误日志等信息  我们主要关注事件分发服务：
 订阅者队列组：每个订阅方（事件接收方）都有自己单独的队列，因此生产者和订阅者队列是一对多的关系。订阅者队列组来管理一个事件的多个订阅队列（实际上订阅队列组在上层还有一个订阅者队列管理器，来统一管理这些订阅队列组，与业务逻辑无关，因此图中未显示）。 订阅者集群组：每个订阅者队列都对应着一个订阅者集群，该集群由多个channel组成，用来加快事件的消费速度。集群具备自动扩容、缩容的功能。 配置中心：配置中心是通过内存来存储着事件的订阅关系。配置中心通过读取或者监听redis的变动，来管理订阅关系。订阅者队列组和订阅者集群组都会读取配置中心并监听配置中心变动，来管理自己的队列或集群。  消息队列中间件的选择 事件分发平台是基于消息队列的思想来构建的，因此需要使用消息队列中间件来管理消息队列。
市面上有许多消息队列中间件，如kafka、rabbitmq等。我们考虑到所需吞吐量并不大，所以初步选择使用redis的列表来实现。使用redis的列表来实现，优势在于能够更快速的完成开发，且整体系统更轻量。
一旦发展到redis的列表不能满足需求时，通过接口或者叫适配器，也能轻松的完成消息队列中间件的切换。
如何监听redis中的事件变动 在事件管理平台将事件订阅关系变动后，会将数据存储到redis中。配置中心如何监听这些数据的变动呢？
每隔一段时间就读取全量数据是最简单的做法，也是最粗暴的做法。因为事件数据有很多，每次读取、对比都需要不少的时间，这就导致事件分发服务对于事件配置的变动很“迟钝”。
我们通过版本号的方法来解决。通过一个hash来存储发送者信息、事件信息、订阅者信息、订阅关系信息的版本号，每次修改这些信息时，都要对其对应的版本号自增。同时，事件分发服务在内存中也会维护这样一个版本号，每隔一段时间（如200ms）读取一次，进行更新，当事件分发服务发现版本号不对的时候，就会去拉取对应的数据，来更新内存中的数据。
这样每次只读取一个很小的hash key即可知道哪些数据需要更新。
消费逻辑 有以下几点需要注意：
 有些订阅者服务需要按照时序来接收事件 系统处于维护状态时，不能接收事件，需要将事件暂存  整体流程图如下：
其中时序功能采用最简单“先到先得”，即按照事件分发服务接收到请求的时间来排序。
可进一步优化的地方 在发送事件时，如果发送失败会进行重试，但是如果超过了重试次数，那么该事件就会丢失。
可考虑在发送失败后报警并每隔一段时间进行发送，直到服务恢复正常，能够正常返回数据时，再继续消费数据。</description>
    </item>
    
    <item>
      <title>统一支付系统</title>
      <link>https://stong1994.github.io/internet/design/basepay/</link>
      <pubDate>Thu, 30 Sep 2021 14:31:11 +0800</pubDate>
      
      <guid>https://stong1994.github.io/internet/design/basepay/</guid>
      <description>背景 已有的支付服务经常出现支付失败、支付状态不准确等问题，且由于历史原因使用的.net开发，维护上有一定困难，因此我们决定重新做一个统一支付系统。
需求 统一支付系统需要满足以下几点需求：
 对接微信、支付宝中的多种支付方式 处理微信、支付宝的回调结果，并通过事件分发平台通知业务方。 开发环境和测试环境要支持1分钱开关，打开开关后，任何支付都是1分钱 支持退款 需要对账功能  架构设计   红色流程为订单的预支付流程。
  黄色流程为用户支付流程，为用户与第三方服务商交互。
  绿色流程为第三方服务商回调流程
  预支付流程  用户在客户端点击商品选择支付 业务系统处理订单逻辑，并调用通用支付系统发起下单请求 通用支付系统调用对应的第三方服务商，获取支付二维码地址或者唤起客户端支付地址，并返回给业务系统，业务系统将其返回给前端 前端接收到地址后，将其转换为二维码或者调换到微信/支付宝客户端支付页面  回调流程   第三方服务商在收到用户支付或者拒绝后，会发送支付结果到回调网关
  回调网关对数据进行解密、校验并将解析出来的数据发往事件分发平台
  由于通用支付系统订阅了该事件，因此事件分发平台会将该事件发送给通用支付系统
  通用支付系统处理支付结果，并将最终的支付状态通过事件分发平台发送给业务子系统
  时序图-以微信的Native支付为例 注意事项 支付和退款分离 由于是统一支付系统，需要兼容各服务商的支付和退款，因此，为了高扩展性，将支付和退款作为两种订单处理，每种都有自己的订单状态
统一支付接口参数 支付宝和微信的支付接口支持非常多的参数，这其中大部分是用不到的，因此在做接口设计时，没必要将这些参数放进去，保持接口的简洁。
统一支付/退款状态 支付宝和微信的支付/退款状态并不同。
 微信有：未支付、已关闭、已撤销、支付失败、支付成功、转入退款、等待扣款 支付宝有：订单创建、交易成功、交易超时或者已全额退款、交易结束  作为统一支付系统，我们需要有自己的一套交易状态来兼容第三方服务商的交易状态。
支付状态：
 交易创建：即未收到任何回调时 交易成功：存在真实的资金流动 交易失败：由于服务商内部服务原因导致交易失败，比如由银行返回的支付失败。 交易关闭：没有真实的资金流动，如交易被撤销，用户付款超时导致交易取消  退款状态：
 退款订单创建 退款关闭 退款成功 退款失败  统一单位 微信支付的最小单位为分，而支付宝的单位为元，支持小数。统一支付接口设计上以分为单位，不支持小数。</description>
    </item>
    
  </channel>
</rss>
